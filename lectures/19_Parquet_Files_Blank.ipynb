{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a128f4d7",
   "metadata": {},
   "source": [
    "# Parquet Files (Optional)\n",
    "\n",
    "As we mentioned in <a href=\"Data File Types.ipynb\">Data File Types</a>, `.parquet` files are a useful data file type. They can help improve query speed, decrease memory requirements, and speed up column based calculations.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Dive a little deeper into the `.parquet` file type,\n",
    "- Learn a bit more about how `.parquet` files improve query performance,\n",
    "- Talk about the `pyarrow` package, and\n",
    "- Demonstrate concepts with an actual `.parquet` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33944e23",
   "metadata": {},
   "source": [
    "In the following notebook we will use a fictional data set, `study_results`, that contains the results of a study examining the effectiveness of three treatments on reducing recovery time in adults. We can load the `.csv` version of these data with `read_csv`. We will eventually look at a parquet equivalent of this file later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e455f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv(\"../data/study_results.csv\")\n",
    "\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f5ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd080f",
   "metadata": {},
   "source": [
    "## Row format compared to columnar format\n",
    "\n",
    "Recall that file types like `.csv`s and `.tsv`s store data in a row-based format like so:\n",
    "\n",
    "<img src=\"row_based.png\" width=\"40%\"></img>\n",
    "\n",
    "and `.parquet` files store their data in a columnar format like so:\n",
    "\n",
    "<img src=\"columnar.png\" width=\"60%\"></img>\n",
    "\n",
    "Above, this columnar format was hidden from us, in a sense, by `pandas` because `read_parquet` automatically converts the `.parquet` file into a `pandas` `DataFrame` which has a row-based approach to storing data. However, the columnar approach gives `.parquet` files its most desirable quality for data analysts and scientists, increased column querying speed.\n",
    "\n",
    "### Increasing column query speed\n",
    "\n",
    "Many real world applications have data sets with many columns (think hundreds or more), but you will commonly only want to use relatively few columns for any given analysis or application. Row-based formats are slow to subselect a set of columns because you need to traverse each row (of which there may be millions) and select the appropriate values. By contrast, with columnar formats you only need to traverse the set of columns and choose the ones you want, this is much faster because there are typically much fewer columns than rows.\n",
    "\n",
    "## Actually directories\n",
    "\n",
    "Up to this point we have thought of parquet files as single files. While there are distinct `.parquet` files, it is more common in practice to deal with the parquet format as a directory of `.parquet` files.\n",
    "\n",
    "### Partitioning\n",
    "\n",
    "For a given data set a single `.parquet` file tends to only store a subset of the data. This subset is formed through partitioning. For example, the study data are split according to sex and then according to treatment group as described in the schematic below.\n",
    "\n",
    "\n",
    "<img src=\"partition.png\" width=\"20%\"></img>\n",
    "\n",
    "We can implement such a partitioning using `to_parquet` along with the `partition_cols` argument which takes in a `list` of column names along which the data are partitioned. Let's do that now with our `study_data.csv` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae49933",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that when we do this we are naming a directory\n",
    "## NOT a file, so there is no .parquet file at the end\n",
    "## of the file name\n",
    "df_csv.to_parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579d1c1",
   "metadata": {},
   "source": [
    "Now if you go to the `data` folder you will see the folder `study_data_parquet` in there. Inside that folder are `sex=M` and `sex=F` folders, each of which contain `treatment=A`, `treatment=B`, `treatment=C` folders as demonstrated in the sequential images below:\n",
    "\n",
    "In the `data` folder:\n",
    "<img src=\"study_parq_folder_1.png\" width=\"90%\"></img>\n",
    "\n",
    "In the `data/study_data_parquet` folder:\n",
    "<img src=\"study_parq_folder_2.png\" width=\"90%\"></img>\n",
    "\n",
    "In the `data/study_data_parquet/sex=F` folder:\n",
    "<img src=\"study_parq_folder_3.png\" width=\"90%\"></img>\n",
    "\n",
    "In the `data/study_data_parquet/sex=F/treatment=A` folder:\n",
    "<img src=\"study_parq_folder_4.png\" width=\"90%\"></img>\n",
    "\n",
    "Even though the parquet \"file\" is a directory you can still read it in using `read_parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911dae9",
   "metadata": {},
   "source": [
    "However, as we can see, this loads the entire data set as a single table, which may be undesirable if the data set is quite large. Luckily, there are alternative ways to loading a parquet file in python.\n",
    "\n",
    "## `pyarrow`\n",
    "\n",
    "One way to take greater advantage of the benefits of parquet is by using the `pyarrow` package, <a href=\"https://arrow.apache.org/docs/python/parquet.html\">https://arrow.apache.org/docs/python/parquet.html</a>, directly. First we need to import `parquet` from `pyarrow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import parquet as pq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea4648",
   "metadata": {},
   "source": [
    "Next we can load the parquet directory with `pq.ParquetDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can read in a directory with\n",
    "## pq.ParquetDataset(directory name)\n",
    "study_directory = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b74a335",
   "metadata": {},
   "source": [
    "We can see how the parquet directory was partitioned using `.partitioning.dictionaries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c6a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f52ed703",
   "metadata": {},
   "source": [
    "You can get the data using the `.read()` function. You can then turn it into a `pandas` `DataFrame` using `.to_pandas()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a pyarrow Table object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55164b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turning a pyarrow table into a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f37d8",
   "metadata": {},
   "source": [
    "If we want to filter the data before it is loaded, we can add a `filters` argument to `pq.ParquetDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640336e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filters takes in a list of tuples\n",
    "## each tuple has a logic condition\n",
    "## (the column string, the logical comparison, the value for comparison)\n",
    "## subsetting is performed in order of the list\n",
    "study_directory_F = pq.ParquetDataset(\"../data/study_data_parquet/\",\n",
    "                                         filters=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## examine the filter in action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5694d",
   "metadata": {},
   "source": [
    "There are also ways to filter after you have loaded the directory, but we will touch on that in the corresponding practice problems notebook. If you are interested in learning more about how you can manage parquet files with `pyarrow`, check out their documentation:\n",
    "- Goes directly to handling parquet files: <a href=\"https://arrow.apache.org/docs/python/parquet.html\">https://arrow.apache.org/docs/python/parquet.html</a>,\n",
    "- Starts at the beginning of the documentation: <a href=\"https://arrow.apache.org/docs/python/getstarted.html\">https://arrow.apache.org/docs/python/getstarted.html</a>.\n",
    "\n",
    "That will be it for this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2694b550",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
